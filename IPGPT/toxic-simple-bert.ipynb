{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":29841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers\n! pip install torch --upgrade\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T06:05:02.490151Z","iopub.execute_input":"2024-07-19T06:05:02.490440Z","iopub.status.idle":"2024-07-19T06:06:39.783226Z","shell.execute_reply.started":"2024-07-19T06:05:02.490397Z","shell.execute_reply":"2024-07-19T06:06:39.782172Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl (4.0MB)\n\u001b[K     |████████████████████████████████| 4.0MB 1.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2019.8.19)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.23)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/22/26b08c841c0493908b4be6960ec2be14a21d1ec0f42ae0cedbca5599ad3d/tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6MB)\n\u001b[K     |████████████████████████████████| 6.6MB 34.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.36.1)\nCollecting packaging>=20.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl (40kB)\n\u001b[K     |████████████████████████████████| 40kB 5.5MB/s  eta 0:00:01\n\u001b[?25hCollecting numpy>=1.17\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8MB)\n\u001b[K     |████████████████████████████████| 14.8MB 911kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.1.2)\nCollecting huggingface-hub<1.0,>=0.1.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/df/1b454741459f6ce75f86534bdad42ca17291b14a83066695f7d2c676e16c/huggingface_hub-0.4.0-py3-none-any.whl (67kB)\n\u001b[K     |████████████████████████████████| 71kB 8.6MB/s  eta 0:00:01\n\u001b[?25hCollecting sacremoses\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/78/fef8d089db5b97546fd6d1ff2e813b8544e85670bf3a8c378c9d0250b98d/sacremoses-0.0.53.tar.gz (880kB)\n\u001b[K     |████████████████████████████████| 880kB 16.1MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (0.6.0)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.2)\nCollecting typing-extensions>=3.7.4.3\n  Downloading https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers) (7.2.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-cp36-none-any.whl size=895254 sha256=58ef200437837718b8b5ab4b7c31197bb819c77ed2fc7e58ea50188cb3833fce\n  Stored in directory: /root/.cache/pip/wheels/56/d5/b2/bc878b2bbddfbcc8fd62ca73c4fd842bd28c1fd3dbdf424c74\nSuccessfully built sacremoses\n\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\n\u001b[31mERROR: tsfresh 0.12.0 has requirement pandas<=0.23.4,>=0.20.3, but you'll have pandas 0.25.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: tensorflow-probability 0.8.0 has requirement cloudpickle==1.1.1, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement imbalanced-learn<0.5,>=0.4.0, but you'll have imbalanced-learn 0.5.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement numpy<1.16,>=1.13, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement scikit-learn<0.21,>=0.19.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: hyperopt 0.2.1 has requirement networkx==2.2, but you'll have networkx 2.4 which is incompatible.\u001b[0m\n\u001b[31mERROR: chainer 6.5.0 has requirement typing-extensions<=3.6.6, but you'll have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.1 which is incompatible.\u001b[0m\nInstalling collected packages: tokenizers, packaging, numpy, typing-extensions, huggingface-hub, sacremoses, transformers\n  Found existing installation: packaging 19.2\n    Uninstalling packaging-19.2:\n      Successfully uninstalled packaging-19.2\n  Found existing installation: numpy 1.16.4\n    Uninstalling numpy-1.16.4:\n      Successfully uninstalled numpy-1.16.4\n  Found existing installation: typing-extensions 3.6.6\n    Uninstalling typing-extensions-3.6.6:\n      Successfully uninstalled typing-extensions-3.6.6\nSuccessfully installed huggingface-hub-0.4.0 numpy-1.19.5 packaging-21.3 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0 typing-extensions-4.1.1\nCollecting torch\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/54/81b1c3c574a1ffde54b0c82ed2a37d81395709cdd5f50e59970aeed5d95e/torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9MB)\n\u001b[K     |████████████████████████████████| 881.9MB 14kB/s s eta 0:00:01    |██▎                             | 63.2MB 34.3MB/s eta 0:00:24     |████▋                           | 126.8MB 58.5MB/s eta 0:00:13     |████████████████████            | 554.1MB 44.5MB/s eta 0:00:08��█████████████████████████▏  | 804.8MB 58.2MB/s eta 0:00:02��█████████████████████████▊  | 819.5MB 58.2MB/s eta 0:00:02\n\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch) (4.1.1)\nRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from torch) (0.7)\n\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.1 which is incompatible.\u001b[0m\nInstalling collected packages: torch\n  Found existing installation: torch 1.3.0\n    Uninstalling torch-1.3.0:\n      Successfully uninstalled torch-1.3.0\nSuccessfully installed torch-1.10.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom typing import Tuple, List\nfrom functools import partial\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:06:39.785508Z","iopub.execute_input":"2024-07-19T06:06:39.785829Z","iopub.status.idle":"2024-07-19T06:06:44.494197Z","shell.execute_reply.started":"2024-07-19T06:06:39.785775Z","shell.execute_reply":"2024-07-19T06:06:44.489961Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 파일 경로 및 BERT 모델 이름 설정\npath = \"./kaggle/input/jigsaw-toxic-comment-classification-challenge/\"\nbert_model_name = 'bert-base-cased'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 데이터 로드\ntrain_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_df, val_df = train_test_split(train_df, test_size=0.05)\n\n# 토크나이저 설정\ntokenizer = BertTokenizer.from_pretrained(bert_model_name)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:06:44.496510Z","iopub.execute_input":"2024-07-19T06:06:44.497131Z","iopub.status.idle":"2024-07-19T06:06:47.343256Z","shell.execute_reply.started":"2024-07-19T06:06:44.496792Z","shell.execute_reply":"2024-07-19T06:06:47.342501Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66b6423c8554b99ab8809abbfbe416a"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Downloading', max=49, style=ProgressStyle(description_width='…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b910d03250e54e9abcfd800fd0190efd"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Downloading', max=570, style=ProgressStyle(description_width=…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299fac7adafd427d9a9dfef06389ce86"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# 데이터셋 클래스 정의\nclass ToxicDataset(Dataset):\n    def __init__(self, tokenizer, dataframe):\n        self.tokenizer = tokenizer\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        tokens = self.tokenizer.encode(row[\"comment_text\"], add_special_tokens=True, max_length=120, truncation=True)\n        x = torch.tensor(tokens)\n        y = torch.tensor(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]], dtype=torch.float)\n        return x, y\n\n# 데이터셋 생성\ntrain_dataset = ToxicDataset(tokenizer, train_df)\nval_dataset = ToxicDataset(tokenizer, val_df)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:06:47.344816Z","iopub.execute_input":"2024-07-19T06:06:47.345129Z","iopub.status.idle":"2024-07-19T06:06:47.489582Z","shell.execute_reply.started":"2024-07-19T06:06:47.345057Z","shell.execute_reply":"2024-07-19T06:06:47.488439Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 데이터로더 생성\ndef collate_fn(batch):\n    x, y = zip(*batch)\n    x = pad_sequence(x, batch_first=True, padding_value=tokenizer.pad_token_id)\n    y = torch.stack(y)\n    return x.to(device), y.to(device)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, sampler=RandomSampler(train_dataset), collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=32, sampler=RandomSampler(val_dataset), collate_fn=collate_fn)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:06:47.492818Z","iopub.execute_input":"2024-07-19T06:06:47.493209Z","iopub.status.idle":"2024-07-19T06:06:47.590286Z","shell.execute_reply.started":"2024-07-19T06:06:47.493149Z","shell.execute_reply":"2024-07-19T06:06:47.589202Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 모델 정의\nclass BertClassifier(nn.Module):\n    def __init__(self, bert_model_name, num_classes):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        cls_output = outputs.pooler_output\n        cls_output = self.classifier(cls_output)\n        return torch.sigmoid(cls_output)\n\nmodel = BertClassifier(bert_model_name, 6).to(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:06:47.592435Z","iopub.execute_input":"2024-07-19T06:06:47.592848Z","iopub.status.idle":"2024-07-19T06:07:04.995873Z","shell.execute_reply.started":"2024-07-19T06:06:47.592777Z","shell.execute_reply":"2024-07-19T06:07:04.995114Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Downloading', max=435779157, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f111889697114ed08b44c3e134fd5ba0"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"# 옵티마이저와 스케줄러 설정\noptimizer = AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * 2\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:07:04.997137Z","iopub.execute_input":"2024-07-19T06:07:04.997387Z","iopub.status.idle":"2024-07-19T06:07:05.022292Z","shell.execute_reply.started":"2024-07-19T06:07:04.997345Z","shell.execute_reply":"2024-07-19T06:07:05.021567Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"# 학습 함수 정의\ndef train(model, loader, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    for x, y in tqdm(loader):\n        optimizer.zero_grad()\n        mask = (x != tokenizer.pad_token_id).float()\n        outputs = model(x, attention_mask=mask)\n        loss = nn.BCELoss()(outputs, y)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    return total_loss / len(loader)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:07:05.023853Z","iopub.execute_input":"2024-07-19T06:07:05.024180Z","iopub.status.idle":"2024-07-19T06:07:05.032023Z","shell.execute_reply.started":"2024-07-19T06:07:05.024124Z","shell.execute_reply":"2024-07-19T06:07:05.031230Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 평가 함수 정의\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    true_labels, predictions = [], []\n    with torch.no_grad():\n        for x, y in tqdm(loader):\n            mask = (x != tokenizer.pad_token_id).float()\n            outputs = model(x, attention_mask=mask)\n            loss = nn.BCELoss()(outputs, y)\n            total_loss += loss.item()\n            true_labels.append(y.cpu().numpy())\n            predictions.append(outputs.cpu().numpy())\n    \n    true_labels = np.concatenate(true_labels)\n    predictions = np.concatenate(predictions)\n    auc_scores = [roc_auc_score(true_labels[:, i], predictions[:, i]) for i in range(6)]\n    return total_loss / len(loader), auc_scores","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:07:05.033346Z","iopub.execute_input":"2024-07-19T06:07:05.033596Z","iopub.status.idle":"2024-07-19T06:07:05.044386Z","shell.execute_reply.started":"2024-07-19T06:07:05.033547Z","shell.execute_reply":"2024-07-19T06:07:05.043464Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# 학습 및 평가\nfor epoch in range(1):\n    print(f'Epoch {epoch + 1}/{2}')\n    train_loss = train(model, train_loader, optimizer, scheduler)\n    val_loss, auc_scores = evaluate(model, val_loader)\n    print(f'Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}')\n    for i, score in enumerate(auc_scores):\n        print(f'Class {i} AUC: {score:.4f}')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T06:07:05.045776Z","iopub.execute_input":"2024-07-19T06:07:05.046066Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/4738 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 3291/4738 [40:03<17:35,  1.37it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# 테스트 데이터 예측\ntest_df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nsubmission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip')\n\nmodel.eval()\nfor i in tqdm(range(len(test_df) // 32 + 1)):\n    batch_df = test_df.iloc[i * 32: (i + 1) * 32]\n    texts = [tokenizer.encode(text, add_special_tokens=True, max_length=120, truncation=True) for text in batch_df[\"comment_text\"]]\n    x = pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n    mask = (x != tokenizer.pad_token_id).float()\n    with torch.no_grad():\n        outputs = model(x, attention_mask=mask).cpu().numpy()\n    submission.iloc[i * 32: (i + 1) * 32, 1:] = outputs\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}